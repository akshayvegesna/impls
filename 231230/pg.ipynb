{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_embed(inp, sd): \n",
    "    wte = sd['transformer.wte.weight']\n",
    "    wpe = sd['transformer.wpe.weight']\n",
    "\n",
    "    token_emb = wte[inp]\n",
    "    pos_emb = wpe[torch.arange(0, len(inp))]\n",
    "    out = token_emb + pos_emb\n",
    "    return out\n",
    "\n",
    "def layer_norm(inputs, weights, bias): \n",
    "    mean = inputs.mean(-1, keepdim=True)\n",
    "    std = inputs.std(-1, keepdim=True)\n",
    "    return (inputs - mean) / (std + 1e-5) * weights + bias\n",
    "\n",
    "\n",
    "def attention(inp, attn_weight, attn_bias, attn_proj_weight, attn_proj_bias): \n",
    "    out = inp @ attn_weight.T + attn_bias\n",
    "    query, key_value = out.split((2048, 2*128), dim=-1) \n",
    "    # q: 4, 2048 \n",
    "    # k: 4, 256\n",
    "    key, value = key_value.split((128, 128), dim=-1) # 4, 128\n",
    "\n",
    "    # run Multi query attention \n",
    "    query_length = query.shape[-1]\n",
    "    query = query.reshape(64, 128)\n",
    "    attn_weights = query @ key.T / 128**0.5 # 64, 4\n",
    "    attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "    attn_out = attn_weights @ value # 64, 128\n",
    "    attn_out = attn_out.reshape(4, 2048)\n",
    "\n",
    "    attn_out = attn_out @ attn_proj_weight.T + attn_proj_bias\n",
    "\n",
    "    return attn_out\n",
    "\n",
    "def apply_mlp(inp, mlp, mlpb, mlp2, mlp2b): \n",
    "    out = inp @ mlp.T + mlpb\n",
    "    out = out @ mlp2.T + mlp2b\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def apply_transformer_block(inp, sd, block_num): \n",
    "    prefix = f'transformer.h.{block_num}.'\n",
    "\n",
    "    ln1 = sd[prefix+'ln_1.weight']\n",
    "    ln1b = sd[prefix+'ln_1.bias']\n",
    "\n",
    "    residual = inp\n",
    "\n",
    "    out = layer_norm(inp, ln1, ln1b)\n",
    "\n",
    "    attn_weight = sd[prefix+'attn.c_attn.weight']\n",
    "    attn_bias = sd[prefix+'attn.c_attn.bias']\n",
    "    attn_proj_weight = sd[prefix+'attn.c_proj.weight']\n",
    "    attn_proj_bias = sd[prefix+'attn.c_proj.bias']\n",
    "\n",
    "    out = attention(out, attn_weight, attn_bias, attn_proj_weight, attn_proj_bias)\n",
    "\n",
    "    out = residual + out\n",
    "\n",
    "    ln2 = sd[prefix+'ln_2.weight']\n",
    "    ln2b = sd[prefix+'ln_2.bias']\n",
    "\n",
    "    out = layer_norm(out, ln2, ln2b)\n",
    "\n",
    "    mlp = sd[prefix+'mlp.c_fc.weight']\n",
    "    mlpb = sd[prefix+'mlp.c_fc.bias']\n",
    "    mlp2 = sd[prefix+'mlp.c_proj.weight']\n",
    "    mlp2b = sd[prefix+'mlp.c_proj.bias']\n",
    "\n",
    "    out = apply_mlp(out, mlp, mlpb, mlp2, mlp2b)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_inference(inp, sd):\n",
    "    out = apply_embed(inp, sd)\n",
    "    for i in range(24): \n",
    "        out = apply_transformer_block(out, sd, i)\n",
    "    out = layer_norm(out, sd['transformer.ln_f.weight'], sd['transformer.ln_f.bias'])\n",
    "    out = out @ sd['lm_head.weight'].T\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bigcode/starcoderbase-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "sd = model.state_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "seq = inputs['input_ids'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10 tokens \n",
    "for i in range(10): \n",
    "    logits = run_inference(seq, sd)\n",
    "    idx = torch.argmax(logits[-1], dim=-1)\n",
    "    seq = torch.cat((seq, idx.unsqueeze(0)), dim=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.h[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten = atten.reshape(ln1_out.shape[0], 16, -1)\n",
    "# move the head to the batch dimension \n",
    "atten = atten.permute(1, 0, 2)\n",
    "# split into keys, queries and values\n",
    "q, k, v = torch.split(atten, 48, dim=-1)\n",
    "\n",
    "# compute the attention score\n",
    "attn_score = torch.matmul(q, k.transpose(-1, -2))\n",
    "attn_score = attn_score / torch.sqrt(torch.tensor(48.0))\n",
    "attn_score = torch.nn.functional.softmax(attn_score, dim=-1)\n",
    "\n",
    "# compute the attention output\n",
    "attn_output = torch.matmul(attn_score, v)\n",
    "attn_output = attn_output.permute(1, 0, 2).reshape(ln1_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln1_out.shape # 4, 2048\n",
    "attn_weight.shape # 2304, 2048\n",
    "attn_bias.shape # 2304\n",
    "\n",
    "# n_heads, seq_len, multi_query = 16, 4, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln1_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output.permute(1, 0, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2304/16/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln1_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0_keys = [k for k in sd.keys() if k.startswith(\"transformer.h.0\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
